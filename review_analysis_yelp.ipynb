{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.environ['JAVA_HOME'] = 'C:\\java\\jdk'\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark-3.5.1-bin-hadoop3\\spark-3.5.1-bin-hadoop3'\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.default.parallelism\", \"100\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc.setLogLevel(\"DEBUG\")  # Set log level to DEBUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp,col\n",
    "reviews_df = spark.read.json('yelp_dataset\\yelp_academic_dataset_review.json').withColumn('Date',to_timestamp(col('date'),'yyyy-MM-dd HH:mm:ss'))\n",
    "business_df = spark.read.json('yelp_dataset\\yelp_academic_dataset_business.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = business_df.drop('attributes','hours')\n",
    "from pyspark.sql.functions import lower\n",
    "business_df = business_df.where(lower(col('Categories')).rlike('.*restaurant.*'))\n",
    "business_id_df = business_df.select(col('business_id')).filter(col('is_open') == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def iqr_outliers(dataframe, column, factor=1.5):\n",
    "    # Calculate quartiles\n",
    "    window = Window.orderBy(column)\n",
    "    df_with_quartiles = dataframe.withColumn(\n",
    "        \"row_number\", F.row_number().over(window)\n",
    "    ).withColumn(\n",
    "        \"total_rows\", F.count(\"*\").over(Window.partitionBy())\n",
    "    )\n",
    "    \n",
    "    q1_row = df_with_quartiles.filter(\n",
    "        F.col(\"row_number\") == (F.col(\"total_rows\") * 0.25).cast(\"int\")\n",
    "    ).select(column).collect()[0][0]\n",
    "    \n",
    "    q3_row = df_with_quartiles.filter(\n",
    "        F.col(\"row_number\") == (F.col(\"total_rows\") * 0.75).cast(\"int\")\n",
    "    ).select(column).collect()[0][0]\n",
    "    \n",
    "    # Calculate IQR and bounds\n",
    "    iqr = q3_row - q1_row\n",
    "    lower_bound = q1_row - factor * iqr\n",
    "    upper_bound = q3_row + factor * iqr\n",
    "    \n",
    "    # Filter outliers\n",
    "    result = dataframe.filter(\n",
    "        (F.col(column) >= lower_bound) & \n",
    "        (F.col(column) <= upper_bound)\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Usage\n",
    "business_df = iqr_outliers(business_df.filter(F.col('is_open') == 1), 'review_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_df = business_df.join(reviews_df,reviews_df.business_id ==  business_df.business_id,\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+--------+-------+----------+-----------+---------------+-----------+------------+-----+-----+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         address|         business_id|          categories|    city|is_open|  latitude|  longitude|           name|postal_code|review_count|stars|state|         business_id|cool|               Date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+----------------+--------------------+--------------------+--------+-------+----------+-----------+---------------+-----------+------------+-----+-----+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   0|2020-07-20 21:02:35|    0|Z4OhvIz9ZYWJE2QWo...|  5.0|It was really goo...|     1|rqYL5uDnaViiBxsN3...|\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   0|2020-07-26 00:41:05|    0|zYQpeciP2QHaHDnuq...|  2.0|Not a good value ...|     0|hFIKF-O20aClIUlbe...|\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   0|2020-11-30 14:00:26|    0|zrkp0HQZ6rtRgJQln...|  1.0|This is one you'r...|     0|eAWnP9ZeVo8kp808y...|\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   1|2020-04-29 22:05:29|    1|LmFNEdzZCpC_Qt_VO...|  2.0|Just picked up th...|     1|YEJKbYDfEmQ5hwUEt...|\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   0|2021-08-05 16:59:06|    0|vENxKldFRoqOSxF2i...|  1.0|Be prepared. If y...|     0|DIK7hbXR8T22pazdM...|\n",
      "+----------------+--------------------+--------------------+--------+-------+----------+-----------+---------------+-----------+------------+-----+-----+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "business_review_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define a UDF (User Defined Function) to calculate sentiment polarity\n",
    "def get_sentiment_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Register the UDF\n",
    "sentiment_udf = udf(get_sentiment_polarity, FloatType())\n",
    "\n",
    "# Apply the UDF to the 'text' column\n",
    "review_sentiment_df = business_review_df.withColumn('sentiment_polarity', sentiment_udf(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+--------+-------+----------+-----------+---------------+-----------+------------+-----+-----+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+------------------+\n",
      "|         address|         business_id|          categories|    city|is_open|  latitude|  longitude|           name|postal_code|review_count|stars|state|         business_id|cool|               Date|funny|           review_id|stars|                text|useful|             user_id|sentiment_polarity|\n",
      "+----------------+--------------------+--------------------+--------+-------+----------+-----------+---------------+-----------+------------+-----+-----+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+------------------+\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   0|2020-07-20 21:02:35|    0|Z4OhvIz9ZYWJE2QWo...|  5.0|It was really goo...|     1|rqYL5uDnaViiBxsN3...|               0.6|\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   0|2020-07-26 00:41:05|    0|zYQpeciP2QHaHDnuq...|  2.0|Not a good value ...|     0|hFIKF-O20aClIUlbe...|        0.06469298|\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   0|2020-11-30 14:00:26|    0|zrkp0HQZ6rtRgJQln...|  1.0|This is one you'r...|     0|eAWnP9ZeVo8kp808y...|       0.094333336|\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   1|2020-04-29 22:05:29|    1|LmFNEdzZCpC_Qt_VO...|  2.0|Just picked up th...|     1|YEJKbYDfEmQ5hwUEt...|      -0.027941177|\n",
      "|8456 Highway 100|G4G7D3_L-dHeq6SXs...|Comfort Food, Foo...|Bellevue|      1|36.0350177|-86.9729834|Biscuit Kitchen|      37221|           8|  3.0|   TN|G4G7D3_L-dHeq6SXs...|   0|2021-08-05 16:59:06|    0|vENxKldFRoqOSxF2i...|  1.0|Be prepared. If y...|     0|DIK7hbXR8T22pazdM...|               0.0|\n",
      "+----------------+--------------------+--------------------+--------+-------+----------+-----------+---------------+-----------+------------+-----+-----+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_sentiment_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side project, use this review text to predict if the review is useful or not, will use LogisticRegression , Decision Tress Classifier and Random Forrest to compare the results!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1833326"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sentiment_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(review_sentiment_df.filter(col('text').isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- is_open: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- sentiment_polarity: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "review_sentiment_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment_polarity: float (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only taking dependant and independant variables\n",
    "model_df = review_sentiment_df.select(col('sentiment_polarity'),col('text'))\n",
    "\n",
    "model_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "model_df = model_df.withColumn(\"label\", when(model_df[\"sentiment_polarity\"] > 0, 1).otherwise(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = model_df.groupBy(\"label\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df_samp = model_df.sample(withReplacement=False, fraction=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 100  # Adjust based on your cluster resources\n",
    "model_df_samp = model_df_samp.repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = model_df_samp.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data, val_data = train_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_data = train_data.cache()\n",
    "\n",
    "# Create a pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\",maxIter=20, tol=1e-4)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(val_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test set accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"label\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion_matrix = predictionAndLabels.groupBy(\"prediction\", \"label\").count().orderBy(\"prediction\", \"label\")\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix.show()\n",
    "\n",
    "# Compute metrics\n",
    "metrics = MulticlassMetrics(predictionAndLabels.rdd.map(tuple))\n",
    "\n",
    "# Overall statistics\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(f\"Precision = {precision}\" )\n",
    "print(f\"Recall = {recall}\")\n",
    "print(f\"F1 Score = {f1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now analyzing Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "model_df = model_df.withColumn(\"sentiment_polarity\", model_df[\"sentiment_polarity\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(useful=0, text='\"Toto, we ain\\'t in Cali anymore.\" The girl in the red slippers doesn\\'t actually say that and no one from California actually calls it by the name. But this pizza place is the bomb! Light on cheese and not overly breaddy or burnt or soggy. Cooked just the right amount! They also serve other things but I only had their pizza and it was bomb diggity for sure. I highly recommend to anyone swinging thru the area.', words=['\"toto,', 'we', \"ain't\", 'in', 'cali', 'anymore.\"', 'the', 'girl', 'in', 'the', 'red', 'slippers', \"doesn't\", 'actually', 'say', 'that', 'and', 'no', 'one', 'from', 'california', 'actually', 'calls', 'it', 'by', 'the', 'name.', 'but', 'this', 'pizza', 'place', 'is', 'the', 'bomb!', 'light', 'on', 'cheese', 'and', 'not', 'overly', 'breaddy', 'or', 'burnt', 'or', 'soggy.', 'cooked', 'just', 'the', 'right', 'amount!', 'they', 'also', 'serve', 'other', 'things', 'but', 'i', 'only', 'had', 'their', 'pizza', 'and', 'it', 'was', 'bomb', 'diggity', 'for', 'sure.', 'i', 'highly', 'recommend', 'to', 'anyone', 'swinging', 'thru', 'the', 'area.'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import DecisionTreeClassifier  \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = model_df.sample(withReplacement=False, fraction=0.1, seed=42).randomSplit([0.8, 0.2], seed=42)\n",
    "train_data, val_data = train_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(train_data)\n",
    "tokenized.head()\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "# hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# dt = DecisionTreeClassifier(maxDepth=4, impurity='gini',featuresCol=\"features\", labelCol=\"useful\")\n",
    "\n",
    "# pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, dt])\n",
    "\n",
    "# # Fit the pipeline\n",
    "# model = pipeline.fit(train_data)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.transform(val_data)\n",
    "\n",
    "# # Evaluate the model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol=\"useful\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(useful=0, text='\"Toto, we ain\\'t in Cali anymore.\" The girl in the red slippers doesn\\'t actually say that and no one from California actually calls it by the name. But this pizza place is the bomb! Light on cheese and not overly breaddy or burnt or soggy. Cooked just the right amount! They also serve other things but I only had their pizza and it was bomb diggity for sure. I highly recommend to anyone swinging thru the area.', words=['\"toto,', 'we', \"ain't\", 'in', 'cali', 'anymore.\"', 'the', 'girl', 'in', 'the', 'red', 'slippers', \"doesn't\", 'actually', 'say', 'that', 'and', 'no', 'one', 'from', 'california', 'actually', 'calls', 'it', 'by', 'the', 'name.', 'but', 'this', 'pizza', 'place', 'is', 'the', 'bomb!', 'light', 'on', 'cheese', 'and', 'not', 'overly', 'breaddy', 'or', 'burnt', 'or', 'soggy.', 'cooked', 'just', 'the', 'right', 'amount!', 'they', 'also', 'serve', 'other', 'things', 'but', 'i', 'only', 'had', 'their', 'pizza', 'and', 'it', 'was', 'bomb', 'diggity', 'for', 'sure.', 'i', 'highly', 'recommend', 'to', 'anyone', 'swinging', 'thru', 'the', 'area.'], filtered=['\"toto,', \"ain't\", 'cali', 'anymore.\"', 'girl', 'red', 'slippers', 'actually', 'say', 'one', 'california', 'actually', 'calls', 'name.', 'pizza', 'place', 'bomb!', 'light', 'cheese', 'overly', 'breaddy', 'burnt', 'soggy.', 'cooked', 'right', 'amount!', 'also', 'serve', 'things', 'pizza', 'bomb', 'diggity', 'sure.', 'highly', 'recommend', 'anyone', 'swinging', 'thru', 'area.'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "removed = remover.transform(tokenized)\n",
    "removed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "# hashedTF = hashingTF.transform(removed)\n",
    "# hashedTF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "# hashedTF.printSchema()\n",
    "# hashedTF.select(\"words\", \"filtered\", \"rawFeatures\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "\n",
    "# Fit the CountVectorizer to create the vocabulary\n",
    "cv_model = cv.fit(removed)\n",
    "\n",
    "# Transform the data to create vector representations\n",
    "vectorized = cv_model.transform(removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# Initialize IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Fit the IDF model\n",
    "idf_model = idf.fit(vectorized)\n",
    "\n",
    "# Transform the data to create TF-IDF vectors\n",
    "tfidf_data = idf_model.transform(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|useful|count|\n",
      "+------+-----+\n",
      "|    12|  128|\n",
      "|    22|   27|\n",
      "|     1|27160|\n",
      "|    13|  105|\n",
      "|     6|  836|\n",
      "|    16|   60|\n",
      "|     3| 5110|\n",
      "|     5| 1429|\n",
      "|    15|   69|\n",
      "|     9|  280|\n",
      "|    17|   37|\n",
      "|     4| 2583|\n",
      "|     8|  389|\n",
      "|    23|   22|\n",
      "|     7|  559|\n",
      "|    10|  201|\n",
      "|    25|   14|\n",
      "|    24|   16|\n",
      "|    21|   31|\n",
      "|    11|  155|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "value_counts = tfidf_data.groupBy(\"useful\").count()\n",
    "\n",
    "# Show the results\n",
    "value_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "dt = DecisionTreeClassifier(maxDepth=4, impurity='gini',featuresCol=\"features\", labelCol=\"useful\")\n",
    "\n",
    "tfidf_data = tfidf_data.withColumn(\"useful\", when(tfidf_data[\"useful\"] > 0, 1).otherwise(0))\n",
    "\n",
    "model = dt.fit(tfidf_data)\n",
    "\n",
    "predictions = model.transform(tfidf_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"useful\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test set accuracy = {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117842"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
